{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xym-hgQwU28B"
   },
   "source": [
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras import Sequential\n",
    "from keras.src.layers import Embedding, Dense, LSTM\n",
    "from keras.src.legacy.preprocessing.text import Tokenizer\n",
    "from keras.src.utils import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#downloads\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hz29Uv1_U5oV",
    "outputId": "0369a0e9-a632-485b-81c4-dba3ff9a6809"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#read data\n",
    "df = pd.read_csv('Sentiment140.tenPercent.sample.tweets.tsv', sep='\\t')\n",
    "df.info()\n",
    "#seed for \"random state\" when training models\n",
    "seed=42"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qbpuBf5U638",
    "outputId": "a4f2a777-ae0e-4e51-96f4-42b1fab9598c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#quick overview of data\n",
    "df.head()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_4uoe_5MU_UM",
    "outputId": "9d23238c-fbf4-4115-fc7e-937bd70045b7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Visualize the amount of 0s and 4s\n",
    "sns.countplot(x='sentiment_label', data=df)\n",
    "#and checking that there are no other unique values\n",
    "unique_sentiments = df['sentiment_label'].unique()\n",
    "print(\"Unique Sentiment Labels:\", unique_sentiments)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "vRKAfNhAjvGw",
    "outputId": "a0061a43-88bf-42b2-a019-66241c78a6ad"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def preprocess_text(sen):\n",
    "\n",
    "    #remove any urls\n",
    "    sentence = re.sub(r'http?://\\S+|www\\.\\S+', '', sen, flags=re.MULTILINE)\n",
    "    #remove non-alphabetical characters\n",
    "    sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    #tokenize\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = [lemmatizer.lemmatize(word) for word in words if word.lower() not in set(stopwords.words('english'))]\n",
    "\n",
    "    return sentence\n"
   ],
   "metadata": {
    "id": "L8wThzKuXqGA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X = []\n",
    "tweets = list(df[\"tweet_text\"])\n",
    "for tweet in tweets:\n",
    "    X.append(preprocess_text(tweet))\n"
   ],
   "metadata": {
    "id": "PT4JH3YEZ2K5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#sentiment values to y\n",
    "y = df[\"sentiment_label\"]\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x>0 else 0, y)))\n",
    "print (y)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6K7Iskmlgbp",
    "outputId": "8c99849e-9302-40a0-bb3d-217eba7b9f61"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#divide into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)"
   ],
   "metadata": {
    "id": "x9y0FtOBaR6A"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#convert the data back to strings\n",
    "X_train_strings = [' '.join(words) for words in X_train]\n",
    "X_test_strings = [' '.join(words) for words in X_test]\n",
    "\n",
    "#tokenize\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train_strings)\n",
    "\n",
    "#data to sequence to train RNN model\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_strings)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_strings)"
   ],
   "metadata": {
    "id": "yQ9k7P9YIsMd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#print sequence for debugging\n",
    "print (X_train_sequences[1])#TA BORT"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kLwAsmPI5_e",
    "outputId": "bda94dd0-a7af-4da2-f6a9-8bd228867310"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#print sequence as test for debugging\n",
    "original_text = tokenizer.sequences_to_texts([X_train_sequences[1]])[0]\n",
    "print(original_text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2VSssBY6ri3",
    "outputId": "74515832-ca5e-4e2f-dc23-7a04266d8742"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "#calculate how much padding needed\n",
    "maxlen = max(max(len(seq) for seq in X_train_sequences), max(len(seq) for seq in X_test_sequences))\n",
    "\n",
    "#pad sequences\n",
    "X_train_sequences = pad_sequences(X_train_sequences, padding=\"post\", maxlen=maxlen)\n",
    "X_test_sequences = pad_sequences(X_test_sequences, padding=\"post\", maxlen=maxlen)"
   ],
   "metadata": {
    "id": "zjjT6sbNLeKl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 16, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(2, activation='softmax'))  # 2 output classes for positive, negative, neutral\n",
    "\n",
    "#create model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#train model\n",
    "model.fit(X_train_sequences, y_train, epochs=4, validation_data=(X_test_sequences, y_test))\n",
    "\n",
    "#evaluate model\n",
    "loss, accuracy_nn = model.evaluate(X_test_sequences, y_test)\n",
    "print(f\"Test Accuracy: {accuracy_nn * 100:.2f}%\")"
   ],
   "metadata": {
    "id": "ncgFXYzfoxxL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "767ddf41-38dd-4e9b-d92c-70c3fe4ff7b0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#predictions\n",
    "predictions = model.predict(X_test_sequences)\n",
    "y_pred_RNN = np.argmax(predictions, axis=1)\n",
    "\n",
    "# create confusion matrix for RNN model\n",
    "conf_matrix_nn = confusion_matrix(y_test, y_pred_RNN)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_nn, annot=True, fmt='d', cmap='YlGn', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "zTv9bGHkfYnb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "outputId": "383e1947-de2a-4e3d-ac6e-1e0bfaba23b4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#data to matrix\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train_strings)\n",
    "X_test_bow = vectorizer.transform(X_test_strings)\n",
    "\n",
    "#Train NB model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_bow, y_train)\n",
    "\n",
    "#preductions\n",
    "y_pred_NB = nb_model.predict(X_test_bow)\n",
    "\n",
    "#evaluate model\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_NB)\n",
    "#classification_rep = classification_report(y_test, y_pred_NB)\n",
    "#print accuracy\n",
    "print(f\"Test Accuracy: {accuracy_nb * 100:.2f}%\")\n"
   ],
   "metadata": {
    "id": "j_V4-YV2xlyP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5da2af82-d9ac-4a3b-b99b-14c14352e77a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#create confusion matrix for nb\n",
    "conf_matrix_nb = confusion_matrix(y_test, y_pred_NB)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_nb, annot=True, fmt='d', cmap='RdPu', xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "AsnGU6NM0fjI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "outputId": "5371a3fb-1623-46ad-bf96-c3ec75826b92"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#create confuison matrixes and plot to compare the 2 models\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(conf_matrix_nb, annot=True, fmt='d', cmap='RdPu', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Naive Bayes Confusion Matrix')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.heatmap(conf_matrix_nn, annot=True, fmt='d', cmap='YlGn', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Neural Network Confusion Matrix')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "methods = ['Naive Bayes', 'RNN']\n",
    "accuracies = [accuracy_nb, accuracy_nn]\n",
    "plt.bar(methods, accuracies, color=['pink', 'green'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"NB Test Accuracy: {accuracy_nb * 100:.2f}%\")\n",
    "print(f\"RNN Test Accuracy: {accuracy_nn * 100:.2f}%\")"
   ],
   "metadata": {
    "id": "PwQS6D996RjN",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "outputId": "d68bcc14-dd11-424d-cbba-a24212bde739"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
