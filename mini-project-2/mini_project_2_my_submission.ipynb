{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c88cefae875512",
   "metadata": {
    "collapsed": false,
    "id": "e1c88cefae875512"
   },
   "source": [
    "# Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80022db4545a8d51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T21:15:08.011180Z",
     "start_time": "2024-06-14T21:15:06.311555Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17670,
     "status": "ok",
     "timestamp": 1708207095458,
     "user": {
      "displayName": "Qing Li",
      "userId": "14082675261611520885"
     },
     "user_tz": -120
    },
    "id": "80022db4545a8d51",
    "outputId": "6835ac89-05ef-4d4e-937e-ade6409fb5bd"
   },
   "outputs": [],
   "source": [
    "!apt install -y enchant-2\n",
    "%pip install pyenchant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f22a05",
   "metadata": {
    "id": "33f22a05"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7070505109ff72d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2613,
     "status": "ok",
     "timestamp": 1708207098320,
     "user": {
      "displayName": "Qing Li",
      "userId": "14082675261611520885"
     },
     "user_tz": -120
    },
    "id": "c7070505109ff72d",
    "outputId": "2f594966-c0d8-4c01-cfcd-f2b1e6684db2",
    "ExecuteTime": {
     "end_time": "2024-06-14T21:29:47.955057Z",
     "start_time": "2024-06-14T21:29:23.588553Z"
    }
   },
   "source": [
    "import re\n",
    "import enchant\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "df = pd.read_csv('Sentiment140.tenPercent.sample.tweets.tsv', delimiter='\\t')\n",
    "# df = pd.read_csv('test.tsv', delimiter='\\t')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liqing/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/liqing/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/liqing/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "32e1160e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "executionInfo": {
     "elapsed": 121813,
     "status": "ok",
     "timestamp": 1708207220129,
     "user": {
      "displayName": "Qing Li",
      "userId": "14082675261611520885"
     },
     "user_tz": -120
    },
    "id": "32e1160e",
    "outputId": "3f7da6d2-b54a-462b-9209-e6f4106a46ce",
    "ExecuteTime": {
     "end_time": "2024-06-14T21:30:47.872577Z",
     "start_time": "2024-06-14T21:29:56.688964Z"
    }
   },
   "source": [
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "USdict = enchant.Dict(\"en_US\")\n",
    "stoplist = set(stopwords.words('english'))\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Clean the text, remove non-English text, and then perform word segmentation and part-of-speech restoration.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'@[\\w]*', '', text)  # Remove the username\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)  # Remove URL\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)  # Remove punctuation\n",
    "    text = text.lower().strip()  # Convert to lowercase and remove spaces at the beginning and end\n",
    "\n",
    "    # Determine whether it is English text.\n",
    "    if not all(ord(c) < 128 for c in text):\n",
    "        return None\n",
    "\n",
    "    # Participle\n",
    "    words = tknzr.tokenize(text.replace('.', ' '))\n",
    "    words = [word for word in words if USdict.check(word) and word.isalpha() and len(word) > 2]\n",
    "\n",
    "    # Part of speech reduction\n",
    "    def get_wordnet_pos(tag):\n",
    "        \"\"\"Obtain WordNet's part-of-speek annotations according to NLTK annotations\"\"\"\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # Default to noun\n",
    "\n",
    "    tagged_words = pos_tag(words)\n",
    "    lemmatized_words = [wnl.lemmatize(tag[0], pos=get_wordnet_pos(tag[1])) for tag in tagged_words]\n",
    "\n",
    "    filtered_words = [word for word in lemmatized_words if word not in stoplist]\n",
    "    resulting_string = ' '.join(filtered_words)\n",
    "    return resulting_string\n",
    "\n",
    "\n",
    "# Apply cleaning and word segmentation functions to the entire data set\n",
    "df['cleaned_tweet_words'] = df['tweet_text'].apply(clean_and_tokenize)\n",
    "df.dropna(subset=['cleaned_tweet_words'], inplace=True)\n",
    "df.head(5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   sentiment_label                                         tweet_text  \\\n",
       "0                4  @elephantbird Hey dear, Happy Friday to You  A...   \n",
       "1                4  Ughhh layin downnnn    Waiting for zeina to co...   \n",
       "2                0  @greeniebach I reckon he'll play, even if he's...   \n",
       "3                0              @vaLewee I know!  Saw it on the news!   \n",
       "4                0  very sad that http://www.fabchannel.com/ has c...   \n",
       "\n",
       "                               cleaned_tweet_words  \n",
       "0           hey dear happy already rice bowl lunch  \n",
       "1                              wait cook breakfast  \n",
       "2  reckon hell play even know nothing wont without  \n",
       "3                                    know saw news  \n",
       "4               sad close one web service use year  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cleaned_tweet_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@elephantbird Hey dear, Happy Friday to You  A...</td>\n",
       "      <td>hey dear happy already rice bowl lunch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Ughhh layin downnnn    Waiting for zeina to co...</td>\n",
       "      <td>wait cook breakfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@greeniebach I reckon he'll play, even if he's...</td>\n",
       "      <td>reckon hell play even know nothing wont without</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@vaLewee I know!  Saw it on the news!</td>\n",
       "      <td>know saw news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad that http://www.fabchannel.com/ has c...</td>\n",
       "      <td>sad close one web service use year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "3398ddbc",
   "metadata": {
    "id": "3398ddbc"
   },
   "source": [
    "# Run Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f8c043e",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1708207220130,
     "user": {
      "displayName": "Qing Li",
      "userId": "14082675261611520885"
     },
     "user_tz": -120
    },
    "id": "1f8c043e",
    "ExecuteTime": {
     "end_time": "2024-06-14T21:32:34.101496Z",
     "start_time": "2024-06-14T21:32:34.088276Z"
    }
   },
   "source": [
    "x = df['cleaned_tweet_words']\n",
    "y = df['sentiment_label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=0.80, min_df=2)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c860ca0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 461341,
     "status": "ok",
     "timestamp": 1708207681469,
     "user": {
      "displayName": "Qing Li",
      "userId": "14082675261611520885"
     },
     "user_tz": -120
    },
    "id": "c860ca0f",
    "outputId": "4fbca48a-0b4e-4a15-e88f-487fa8f308fe",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-14T21:32:38.347110Z"
    }
   },
   "source": [
    "\n",
    "classifiers = {\n",
    "    \"KNN Classifier\": KNeighborsClassifier(),\n",
    "    # \"Linear Regrerssion lbfgs\": LogisticRegression(solver='lbfgs', max_iter=1000),\n",
    "    # \"Linear Regrerssion saga\": LogisticRegression(solver='saga', max_iter=1000),\n",
    "    # \"Linear Regrerssion lbfgs with c = 0.1\": LogisticRegression(solver='lbfgs', max_iter=1000, C=0.1),\n",
    "    # \"Random Forest\": RandomForestClassifier(),\n",
    "    # \"SVM Classifier\": SGDClassifier(loss='hinge'), # SVM\n",
    "    \"Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "classifiers_names = []\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    pipe = make_pipeline(bow_vectorizer, clf)\n",
    "    pipe.fit(x_train, y_train)\n",
    "\n",
    "    # Calculate the accuracy of training\n",
    "    train_pred = pipe.predict(x_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Traning Accuracy of {name}:\\n{classification_report(y_train, train_pred)}\\n\")\n",
    "\n",
    "    # Calculate the accuracy of the test\n",
    "    test_pred = pipe.predict(x_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    classifiers_names.append(name)\n",
    "    print(f\"Test Accuracy of {name}:\\n{classification_report(y_test, test_pred)}\\n\")\n",
    "\n",
    "# Draw a chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(classifiers_names, train_accuracies, label='Training Accuracy', marker='o')\n",
    "plt.plot(classifiers_names, test_accuracies, label='Test Accuracy', marker='o')\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classifier Training and Test Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "50007de1a86b9550"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
