{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49d0af89e4543347"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install the required libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1c88cefae875512"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pandas scikit-learn matplotlib seaborn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T10:45:46.996351Z",
     "start_time": "2024-01-31T10:45:45.776447Z"
    }
   },
   "id": "80022db4545a8d51",
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T10:45:46.996763Z",
     "start_time": "2024-01-31T10:45:46.989767Z"
    }
   },
   "id": "a33ad8dfcc8c729",
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "df = pd.read_csv('bank-additional-full.csv', delimiter=';')\n",
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "# Select relevant features. Assuming all features are relevant for the first iteration.\n",
    "features = df.drop('y', axis=1)  # Drop the target column\n",
    "selected_columns = ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan']\n",
    "features = features[selected_columns]\n",
    "\n",
    "target = df['y']  # Target column\n",
    "# Handling missing values\n",
    "# Assuming no missing values for this example. If there are, you can handle them with methods like fillna() or dropna().\n",
    "features.bfill(inplace=True)\n",
    "# Show the first few rows of the processed data\n",
    "features.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T11:29:27.303013Z",
     "start_time": "2024-01-31T11:29:27.197683Z"
    }
   },
   "id": "52675fcf024830f4",
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "\n",
    "1. **Age (Numeric)**: \n",
    "   - **Relevance**: Age is a crucial factor in financial planning. Younger clients may be more risk-tolerant and interested in growth-oriented investments, while older clients might prefer safer, income-generating options. Understanding the age distribution of clients can help tailor financial advice and product offerings.\n",
    "\n",
    "2. **Job (Categorical)**: \n",
    "   - **Relevance**: A client's occupation can significantly influence their income level, financial goals, and risk appetite. For example, entrepreneurs might have variable income and a higher risk tolerance, whereas government employees might have stable income and prefer secure investments. This data can be used to develop targeted financial products and advice.\n",
    "\n",
    "3. **Marital Status (Categorical)**:\n",
    "   - **Relevance**: Marital status can impact financial responsibilities and goals. Married clients might be more interested in joint investments or long-term financial planning for family needs, while single clients may have different priorities. This information can help in understanding the client's financial commitments and planning needs.\n",
    "\n",
    "4. **Education (Categorical)**:\n",
    "   - **Relevance**: Education level can correlate with financial literacy, income potential, and investment preferences. Highly educated clients might be more inclined towards sophisticated investment options, while others might prefer simple, straightforward products. Tailoring communication and advice based on education levels can improve client engagement and satisfaction.\n",
    "\n",
    "5. **Default on Credit (Categorical)**:\n",
    "   - **Relevance**: A history of defaulting on credit can be a critical indicator of a client's financial health and creditworthiness. Clients with a history of default might require more cautious financial planning and might not be suitable for certain types of credit or investment products.\n",
    "\n",
    "6. **Housing Loan (Categorical)**:\n",
    "   - **Relevance**: Whether a client has a housing loan can inform their financial liabilities and risk profile. Clients with significant housing loans might have less disposable income for investments and might prefer safer, liquid assets.\n",
    "\n",
    "7. **Personal Loan (Categorical)**:\n",
    "   - **Relevance**: Similar to housing loans, personal loans can affect a client's financial flexibility. A high level of personal debt might indicate a need for debt management advice and could affect the suitability of certain investment products.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbaf564f0c5b95e7"
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "# Separate numeric columns and classification columns\n",
    "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan']\n",
    "numerical_columns_train = X_train.columns.difference(categorical_columns)\n",
    "numerical_columns_test = X_test.columns.difference(categorical_columns)\n",
    "\n",
    "# Apply label coding to classified data\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    X_test[col] = label_encoder.fit_transform(X_test[col])\n",
    "\n",
    "# Application standardization of numerical data\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_columns_train] = scaler.fit_transform(X_train[numerical_columns_train])\n",
    "X_test[numerical_columns_train] = scaler.fit_transform(X_test[numerical_columns_train])\n",
    "\n",
    "\n",
    "# Standardize the features (important for models like KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Model Training\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T11:32:38.490738Z",
     "start_time": "2024-01-31T11:32:38.419242Z"
    }
   },
   "id": "55392bf0ea8dbe7e",
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# K-Nearest Neighbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "# Evaluate Logistic Regression\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
    "lr_confusion_matrix = confusion_matrix(y_test, lr_predictions)\n",
    "\n",
    "# Evaluate K-Nearest Neighbors\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "knn_confusion_matrix = confusion_matrix(y_test, knn_predictions)\n",
    "\n",
    "# Visualization (Confusion Matrix for Logistic Regression)\n",
    "sns.heatmap(lr_confusion_matrix, annot=True)\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.ylabel('Actual labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show()\n",
    "\n",
    "# Visualization (Confusion Matrix for K-Nearest Neighbors)\n",
    "sns.heatmap(knn_confusion_matrix, annot=True)\n",
    "plt.title('K-Nearest Neighbors Confusion Matrix')\n",
    "plt.ylabel('Actual labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show()\n",
    "\n",
    "# Output accuracy scores\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy}\")\n",
    "print(f\"K-Nearest Neighbors Accuracy: {knn_accuracy}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T11:33:06.937026Z",
     "start_time": "2024-01-31T11:33:06.344445Z"
    }
   },
   "id": "32eb39c9f7a23115",
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation\n",
    "### K-Nearest Neighbors Confusion Matrix:\n",
    "\n",
    "- **True Negative (TN)**: The top left square (beige color) shows the number of negatives correctly classified as negative (class 0). In this case, around 7,100 instances were correctly predicted as not subscribing to a term deposit.\n",
    "- **False Positive (FP)**: The top right square (beige color) indicates the number of negatives incorrectly classified as positive (class 1). About 160 instances were incorrectly predicted as subscribing to a term deposit.\n",
    "- **False Negative (FN)**: The bottom left square (dark purple) shows the number of positives incorrectly classified as negative. Here, around 880 instances that did subscribe were incorrectly predicted as not subscribing.\n",
    "- **True Positive (TP)**: The bottom right square (dark purple) represents the number of positives correctly classified. There are 59 instances correctly identified as subscriptions.\n",
    "\n",
    "### Logistic Regression Confusion Matrix:\n",
    "\n",
    "- **True Negative (TN)**: The top left square indicates a similar count of true negatives as KNN, around 7,300.\n",
    "- **False Positive (FP)**: The top right square shows that there are no false positives; the model did not incorrectly predict any subscriptions.\n",
    "- **False Negative (FN)**: The bottom left square shows that there are around 940 false negatives, meaning the model missed these subscriptions and predicted them as non-subscriptions.\n",
    "- **True Positive (TP)**: The bottom right square shows that there are no true positives; the model did not correctly predict any subscriptions.\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- Both models appear to have a high number of false negatives, especially the Logistic Regression model, which failed to identify any of the positive cases.\n",
    "- The KNN model managed to identify some true positives, but the number is quite low.\n",
    "- The imbalance between the classes could be an issue here, as indicated by the high number of true negatives and low number of true positives, which suggests that the dataset might be imbalanced with a much larger number of negative instances (no subscription) than positive ones (subscription).\n",
    "- Logistic Regression seems to predict that no one will subscribe, which might suggest a model that is not well-calibrated or a dataset that is not well-suited for this model without further processing or feature engineering.\n",
    "- The KNN model, while still heavily biased towards predicting non-subscriptions, at least identified some true subscriptions.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "- Look into the imbalance in the dataset. If the number of non-subscriptions is much higher than the number of subscriptions, consider techniques such as oversampling, undersampling, or using algorithms that handle imbalance well.\n",
    "- Perform feature engineering to try to improve the model's ability to distinguish between the two classes.\n",
    "- Adjust the decision threshold for the models to see if that helps reduce the number of false negatives."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65c1ec8ef8109904"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
